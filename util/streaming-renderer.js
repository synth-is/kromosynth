/**
 * StreamingRenderer - Adaptive Suspend/Resume rendering with incremental capture
 *
 * Provides progressive audio generation with millisecond-range latency by:
 * 1. Measuring render performance (RTF - Real-Time Factor)
 * 2. Adaptively sizing chunks based on RTF to hit target latency
 * 3. Using suspend/resume to pause rendering at chunk boundaries
 * 4. Capturing audio incrementally via AudioWorklet
 * 5. Emitting chunks progressively for immediate playback
 *
 * IMPORTANT: Uses THE SAME rendering path as batch mode (maintains parity).
 *
 * Performance Examples:
 * - RTF = 0.01 (100x faster): 5s chunks ‚Üí 50ms latency ‚ö°‚ö°‚ö°
 * - RTF = 0.05 (20x faster):  5s chunks ‚Üí 250ms latency ‚ö°‚ö°
 * - RTF = 0.2  (5x faster):   5s chunks ‚Üí 1s latency ‚ö°
 * - RTF = 1.0  (real-time):   1s chunks ‚Üí 1s latency
 * - RTF = 5.0  (5x slower):   0.2s chunks ‚Üí 1s latency
 *
 * Result: 100% identical output to batch mode (RMSE: 0.0) with progressive delivery
 */

import isString from 'lodash-es/isString.js';
import NodeWebAudioAPI from 'node-web-audio-api';
const { OfflineAudioContext, AudioWorkletNode } = NodeWebAudioAPI;
import { CAPTURE_PROCESSOR_CODE } from './capture-processor.js';

export class StreamingRenderer {
  /**
   * Create a new streaming renderer
   *
   * @param {AudioContext} audioContext - Web Audio API context
   * @param {number} sampleRate - Target sample rate (e.g., 48000)
   * @param {Object} options - Optional configuration
   * @param {boolean} options.useGPU - Enable GPU acceleration for CPPN (default: true)
   * @param {boolean} options.measureRTF - Measure RTF before rendering (adds ~3s latency) (default: false)
   * @param {number} options.defaultChunkDuration - Chunk size when not measuring RTF (default: 0.5s)
   * @param {number} options.targetLatency - Target time to first sound in seconds (default: 1.0)
   * @param {number} options.minChunkDuration - Minimum chunk size in seconds (default: 0.1)
   * @param {number} options.maxChunkDuration - Maximum chunk size in seconds (default: 5.0)
   * @param {boolean} options.enableAdaptiveChunking - Use adaptive chunk sizing (default: true)
   * @param {boolean} options.controlledResume - Enable client-controlled resume (default: false)
   * @param {number} options.initialBufferDuration - Initial buffer size for controlled resume (default: 2.0s)
   * @param {number} options.bufferAhead - How far ahead to stay in controlled mode (default: 2.0s)
   */
  constructor(audioContext, sampleRate, options = {}) {
    this.audioContext = audioContext;
    this.sampleRate = sampleRate;
    this.useGPU = options.useGPU !== undefined ? options.useGPU : true;

    // RTF measurement configuration
    this.measureRTF = options.measureRTF !== undefined ? options.measureRTF : false;
    this.defaultChunkDuration = options.defaultChunkDuration || 0.5;  // 500ms default

    // Adaptive chunking configuration
    this.targetLatency = options.targetLatency || 1.0;  // 1 second to first sound
    this.minChunkDuration = options.minChunkDuration || 0.1;  // 100ms minimum
    this.maxChunkDuration = options.maxChunkDuration || 5.0;  // 5s maximum
    this.enableAdaptiveChunking = options.enableAdaptiveChunking !== false;

    // Client-controlled resume configuration
    this.controlledResume = options.controlledResume || false;
    this.initialBufferDuration = options.initialBufferDuration || 2.0;  // 2s initial buffer
    this.bufferAhead = options.bufferAhead || 2.0;  // Stay 2s ahead
  }

  /**
   * Render genome to audio with adaptive chunking
   *
   * Phase 1: RTF measurement + adaptive chunk calculation (current)
   * Phase 2: Suspend/resume + AudioWorklet (coming next)
   *
   * @param {Object} genomeAndMeta - Genome and metadata
   * @param {number} duration - Duration in seconds
   * @param {OfflineAudioContext} offlineContext - OfflineAudioContext for rendering
   * @param {Object} options - Rendering options
   * @param {Function} options.onChunk - Callback for progressive chunks (chunk: Float32Array) => void
   * @param {Function} options.onProgress - Progress callback (progress: {rendered, total, rtf}) => void
   * @param {Function} options.shouldResume - (renderedDuration) => boolean - Return true to resume rendering (controlled mode)
   * @param {Function} options.onBufferFull - (renderedDuration) => void - Called when initial buffer is complete (controlled mode)
   * @returns {Promise<AudioBuffer>} - Final rendered audio buffer
   */
  async render(genomeAndMeta, duration, offlineContext, options = {}) {
    const { onChunk, onProgress, shouldResume, onBufferFull } = options;

    console.log('üéµ StreamingRenderer: Starting adaptive render');

    // Extract genome
    let genome;
    if (isString(genomeAndMeta.genome)) {
      const { getGenomeFromGenomeString } = await import('./genome-import.js');
      genome = await getGenomeFromGenomeString(genomeAndMeta.genome);
    } else {
      genome = genomeAndMeta.genome;
    }

    const actualDuration = duration || genomeAndMeta.duration || 4.0;
    const noteDelta = genomeAndMeta.noteDelta || 0;
    const velocity = genomeAndMeta.velocity || 1.0;
    const reverse = genomeAndMeta.reverse || false;

    // If no onChunk callback or adaptive chunking disabled, use batch mode
    if (!onChunk || !this.enableAdaptiveChunking) {
      console.log('  ‚Üí Using batch mode (no streaming)');
      return this._renderBatchMode(genome, genomeAndMeta, actualDuration, offlineContext);
    }

    // Phase 1: Measure RTF and calculate chunk size (optional)
    let chunkDuration;
    if (this.measureRTF) {
      const rtf = await this._measureRTF(genome, genomeAndMeta);
      chunkDuration = this._calculateOptimalChunkSize(rtf);
      console.log(`üìä RTF: ${rtf.toFixed(3)}x (${rtf < 1 ? 'faster' : 'slower'} than real-time)`);
      console.log(`‚öôÔ∏è  Optimal chunk size: ${chunkDuration.toFixed(2)}s`);
      console.log(`‚è±Ô∏è  Expected latency to first sound: ${(chunkDuration * rtf * 1000).toFixed(0)}ms`);
    } else {
      chunkDuration = this.defaultChunkDuration;
      console.log(`‚öôÔ∏è  Using default chunk size: ${chunkDuration.toFixed(2)}s (RTF measurement skipped for faster startup)`);
    }

    // Phase 2: Suspend/resume + AudioWorklet capture
    return this._renderWithSuspendResume(
      genome,
      actualDuration,
      noteDelta,
      velocity,
      reverse,
      offlineContext,
      chunkDuration,
      onChunk,
      onProgress,
      shouldResume,
      onBufferFull
    );
  }

  /**
   * Measure Real-Time Factor by rendering a small test chunk
   *
   * @private
   * @returns {Promise<number>} RTF value (e.g., 0.1 = 10x faster than real-time)
   */
  async _measureRTF(genome, genomeAndMeta) {
    const testDuration = 0.5; // 500ms test chunk

    console.log('üìä Measuring render performance...');
    const startTime = performance.now();

    // Create temporary context for measurement
    const testContext = new OfflineAudioContext({
      numberOfChannels: 1,
      length: Math.round(this.sampleRate * testDuration),
      sampleRate: this.sampleRate
    });

    // Render test chunk using same path as batch
    const { renderAudioAndSpectrogram } = await import('./render.js');
    const noteDelta = genomeAndMeta.noteDelta || 0;
    const velocity = genomeAndMeta.velocity || 1.0;
    const reverse = genomeAndMeta.reverse || false;

    await renderAudioAndSpectrogram(
      genome.asNEATPatch,
      genome.waveNetwork,
      testDuration,
      noteDelta,
      velocity,
      this.sampleRate,
      reverse,
      false, // asDataArray
      testContext,
      this.audioContext,
      false, // useOvertoneInharmonicityFactors
      this.useGPU,
      false, // antiAliasing
      false  // frequencyUpdatesApplyToAllPathcNetworkOutputs
    );

    const renderTime = (performance.now() - startTime) / 1000; // Convert to seconds
    const rtf = renderTime / testDuration;

    return rtf;
  }

  /**
   * Calculate optimal chunk size based on RTF to hit target latency
   *
   * @private
   * @param {number} rtf - Real-Time Factor
   * @returns {number} Optimal chunk duration in seconds
   */
  _calculateOptimalChunkSize(rtf) {
    // Target latency = chunkDuration * rtf
    // Therefore: chunkDuration = targetLatency / rtf
    let chunkDuration = this.targetLatency / rtf;

    // Clamp to min/max bounds
    chunkDuration = Math.max(
      this.minChunkDuration,
      Math.min(this.maxChunkDuration, chunkDuration)
    );

    return chunkDuration;
  }

  /**
   * Batch mode rendering (maintains parity with batch renderer)
   *
   * @private
   */
  async _renderBatchMode(genome, genomeAndMeta, actualDuration, offlineContext) {
    const { waveNetwork } = genome;
    const noteDelta = genomeAndMeta.noteDelta || 0;
    const velocity = genomeAndMeta.velocity || 1.0;
    const reverse = genomeAndMeta.reverse || false;

    // Use the EXACT SAME rendering function as batch mode
    const { renderAudioAndSpectrogram } = await import('./render.js');

    const audioBufferAndCanvas = await renderAudioAndSpectrogram(
      genome.asNEATPatch,
      waveNetwork,
      actualDuration,
      noteDelta,
      velocity,
      this.sampleRate,
      reverse,
      false, // asDataArray
      offlineContext,
      this.audioContext,
      false, // useOvertoneInharmonicityFactors
      this.useGPU,
      false, // antiAliasing
      false  // frequencyUpdatesApplyToAllPathcNetworkOutputs
    );

    const audioBuffer = audioBufferAndCanvas ? audioBufferAndCanvas.audioBuffer : null;

    console.log('  ‚úì Render complete (100% identical to batch mode)');
    return audioBuffer;
  }

  /**
   * Render with suspend/resume + AudioWorklet for progressive delivery
   *
   * @private
   */
  async _renderWithSuspendResume(
    genome,
    duration,
    noteDelta,
    velocity,
    reverse,
    offlineContext,
    chunkDuration,
    onChunk,
    onProgress,
    shouldResume,
    onBufferFull
  ) {
    const { waveNetwork } = genome;
    const numChunks = Math.ceil(duration / chunkDuration);

    console.log(`üé¨ Starting suspend/resume render (${numChunks} chunks of ${chunkDuration}s)`);

    // 1. Load AudioWorklet on offlineContext (required - can't use different context)
    let workletLoadTime = 0;

    // IMPORTANT: Must load on offlineContext where rendering happens
    // (audioContext is only for GPU CPPN computation, not for audio graph)
    if (!offlineContext.audioWorklet) {
      throw new Error('offlineContext does not have audioWorklet property. This may be a node-web-audio-api version issue.');
    }

    console.log('üì¶ Loading AudioWorklet...');
    const workletLoadStart = performance.now();
    const blob = new Blob([CAPTURE_PROCESSOR_CODE], {
      type: 'application/javascript'
    });
    const url = URL.createObjectURL(blob);
    await offlineContext.audioWorklet.addModule(url);
    URL.revokeObjectURL(url);
    workletLoadTime = performance.now() - workletLoadStart;
    console.log(`  ‚úì Loaded in ${workletLoadTime.toFixed(1)}ms`);

    // 2. Create capture node on offlineContext
    console.log('  Creating AudioWorkletNode on offlineContext');
    const captureNode = new AudioWorkletNode(
      offlineContext,
      'capture-processor'
    );

    // Add error handler to prevent crashes
    captureNode.onprocessorerror = (event) => {
      console.log('  ‚ÑπÔ∏è  AudioWorklet processor ended (expected at end of render)');
      // Don't throw - this is normal when rendering completes
    };

    // 3. Set up chunk collection
    const capturedChunks = [];
    let firstChunkTime = null;
    const renderSetupStart = performance.now();

    captureNode.port.onmessage = (event) => {
      const { type, data, totalCaptured } = event.data;
      if (type === 'audioChunk') {
        capturedChunks.push(data);
        // Verbose logging disabled - causes audio distortion
        // console.log(`  ‚Üê Captured chunk ${capturedChunks.length}: ${data.length} samples (total: ${totalCaptured})`);

        // Track first chunk timing
        if (firstChunkTime === null) {
          firstChunkTime = performance.now() - renderSetupStart;
        }

        // Emit chunk to callback
        if (onChunk) {
          onChunk(data);
        }

        // Report progress
        if (onProgress) {
          const totalSamples = Math.round(this.sampleRate * duration);
          onProgress({
            rendered: totalCaptured,
            total: totalSamples,
            progress: totalCaptured / totalSamples
          });
        }
      }
    };

    // 4. Schedule suspends with controlled resume support
    const initialChunks = this.controlledResume
      ? Math.ceil(this.initialBufferDuration / chunkDuration)
      : numChunks;

    if (this.controlledResume) {
      console.log(`üìä Controlled resume: initial buffer = ${this.initialBufferDuration}s (${initialChunks} chunks), buffer ahead = ${this.bufferAhead}s`);
    }

    for (let i = 1; i < numChunks; i++) {
      const suspendTime = i * chunkDuration;
      const isInitialBuffer = i < initialChunks;

      offlineContext.suspend(suspendTime).then(async () => {
        // Check for parameter updates before resuming
        if (options.getUpdatedParams) {
          const updatedParams = options.getUpdatedParams();
          if (updatedParams && (
            updatedParams.noteDelta !== noteDelta ||
            updatedParams.velocity !== velocity ||
            updatedParams.duration !== duration
          )) {
            console.log(`üîÑ Parameter update at ${suspendTime.toFixed(2)}s:`, updatedParams);
            // Update local parameters for next iterations
            if (updatedParams.noteDelta !== undefined) noteDelta = updatedParams.noteDelta;
            if (updatedParams.velocity !== undefined) velocity = updatedParams.velocity;
            if (updatedParams.duration !== undefined) duration = updatedParams.duration;
            
            // Note: The audio graph is already wired up and running. 
            // We can't rewire it mid-render without major surgery.
            // This is a limitation of the current architecture.
            // TODO: To support true parameter updates, we'd need to modify the audio graph nodes
            // or use AudioParams with automation, which would require deeper changes.
          }
        }

        if (isInitialBuffer) {
          // Auto-resume for initial buffer
          offlineContext.resume();
        } else {
          // Beyond initial buffer - notify and wait for permission
          if (i === initialChunks && onBufferFull) {
            onBufferFull(suspendTime);
          }

          // Wait for shouldResume callback to allow continuation
          if (shouldResume) {
            // Poll shouldResume until it returns true
            const checkResume = () => {
              if (shouldResume(suspendTime)) {
                offlineContext.resume();
              } else {
                // Check again in 100ms
                setTimeout(checkResume, 100);
              }
            };
            checkResume();
          } else {
            // No shouldResume callback - auto-resume (fallback)
            offlineContext.resume();
          }
        }
      });
    }

    // 5. Start rendering (with captureNode injected)
    console.log('üéµ Starting progressive render...');
    const { renderAudioAndSpectrogram } = await import('./render.js');

    const startTime = performance.now();

    let audioBufferAndCanvas;
    try {
      // renderAudioAndSpectrogram calls startRendering() internally
      audioBufferAndCanvas = await renderAudioAndSpectrogram(
        genome.asNEATPatch,
        waveNetwork,
        duration,
        noteDelta,
        velocity,
        this.sampleRate,
        reverse,
        false, // asDataArray
        offlineContext,
        this.audioContext,
        false, // useOvertoneInharmonicityFactors
        this.useGPU,
        false, // antiAliasing
        false, // frequencyUpdatesApplyToAllPathcNetworkOutputs
        null,  // sampleCountToActivate
        null,  // sampleOffset
        captureNode  // ‚Üê Inject capture node!
      );
    } catch (error) {
      // Check if this is just the AudioWorklet cleanup error
      if (error.message && error.message.includes('expect Object, got: Undefined')) {
        console.log('  ‚ÑπÔ∏è  AudioWorklet cleanup error (expected, can be ignored)');
        // Rendering actually completed successfully - continue
      } else {
        // Real error - rethrow
        throw error;
      }
    }

    const renderTime = (performance.now() - startTime) / 1000;

    // Audio fully captured - no need to flush

    const audioBuffer = audioBufferAndCanvas ? audioBufferAndCanvas.audioBuffer : null;

    console.log(`\n‚úÖ Suspend/resume render complete in ${renderTime.toFixed(2)}s`);
    console.log(`   Total chunks emitted: ${capturedChunks.length}`);
    console.log();
    console.log('‚è±Ô∏è  Timing Breakdown:');
    console.log(`   AudioWorklet load:  ${workletLoadTime.toFixed(1)}ms (required on offlineContext, can't be pre-loaded)`);
    if (firstChunkTime !== null) {
      const actualRenderTime = firstChunkTime - workletLoadTime;
      console.log(`   First chunk render: ${actualRenderTime.toFixed(1)}ms (CPPN init + audio graph + render)`);
      console.log(`   Total to first chunk: ${firstChunkTime.toFixed(1)}ms`);
    }

    return audioBuffer;
  }

}
